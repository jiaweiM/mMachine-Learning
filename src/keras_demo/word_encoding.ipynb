{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": "'2.7.0'"
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import tensorflow.keras as keras\n",
    "\n",
    "keras.__version__"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "# 单词级的 one-hot 编码"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "source": [
    "import numpy as np\n",
    "\n",
    "samples = ['The cat sat on the mat.', 'The dog ate my homework.']  # 列表中每个元素对应一个样本\n",
    "\n",
    "token_index = {}  # 构建数据中所有标记的索引\n",
    "for sample in samples:\n",
    "    for word in sample.split():  # 利用 split 对样本进行分词。在实际应用中，还需要从样本中去掉标点符号和特殊字符\n",
    "        if word not in token_index:\n",
    "            token_index[word] = len(token_index) + 1  # 为每个唯一单词指定一个唯一索引，这里没有为索引 0 指定单词\n",
    "\n",
    "max_length = 10  # 对样本进行分词。只考虑每个样本前 max_length 个单词\n",
    "results = np.zeros(shape=(len(samples),  # 结果保存此处, batch, sequence_length, one-hot\n",
    "                          max_length,\n",
    "                          max(token_index.values()) + 1))\n",
    "\n",
    "for i, sample in enumerate(samples):\n",
    "    for j, word in list(enumerate(sample.split()))[:max_length]:\n",
    "        index = token_index.get(word)\n",
    "        results[i, j, index] = 1.0"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "# 用 keras 实现单词级 one-hot 编码"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from keras.preprocessing.text import Tokenizer\n",
    "\n",
    "samples = ['The cat sat on the mat.', 'The dog ate my homework.']\n",
    "\n",
    "tokenizer = Tokenizer(num_words=1000)  # 创建分词器，设置为只考虑前 1000 个最常见的单词\n",
    "tokenizer.fit_on_texts(samples)  # 构建单词索引\n",
    "\n",
    "sequences = tokenizer.texts_to_sequences(samples)  # 将字符串转换为整数索引组成的列表\n",
    "\n",
    "one_hot_results = tokenizer.texts_to_matrix(samples, mode='binary')\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}