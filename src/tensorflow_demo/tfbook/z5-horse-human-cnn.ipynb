{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 下载数据\n",
    "\n",
    "图片按照如下目录结构组织：\n",
    "\n",
    "![](images/2022-01-13-16-08-29.png)\n",
    "\n",
    "方便 `ImageDataGenerator` 提取数据。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import urllib.request\n",
    "import zipfile\n",
    "\n",
    "url = (\n",
    "    \"https://storage.googleapis.com/laurencemoroney-blog.appspot.com/horse-or-human.zip\"\n",
    ")\n",
    "\n",
    "file_name = \"horse-or-human.zip\"\n",
    "training_dir = \"horse-or-human/training/\"\n",
    "urllib.request.urlretrieve(url, file_name)\n",
    "\n",
    "zip_ref = zipfile.ZipFile(file_name, \"r\")\n",
    "zip_ref.extractall(training_dir)\n",
    "zip_ref.close()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 1027 images belonging to 2 classes.\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.preprocessing import image\n",
    "\n",
    "# 所有图片缩放到 1./255\n",
    "train_datagen = image.ImageDataGenerator(rescale=1 / 255.0)\n",
    "train_generator = train_datagen.flow_from_directory(\n",
    "    training_dir, \n",
    "    target_size=(300, 300), \n",
    "    class_mode=\"binary\"\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "和 Fashion MNIST 不同的是，这里的图片是 300x300 的彩色图片，所以可能需要更多层，通道数也从 1 到 3.\n",
    "\n",
    "另外，只有两种类型，因此输出神经元只需要一个。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow.keras as keras\n",
    "\n",
    "model = keras.models.Sequential(\n",
    "    [\n",
    "        keras.layers.Conv2D(\n",
    "            16, (3, 3), activation=\"relu\", input_shape=(300, 300, 3)\n",
    "        ),  # 底层特征较少\n",
    "        keras.layers.MaxPooling2D(2, 2),\n",
    "        keras.layers.Conv2D(32, (3, 3), activation=\"relu\"),\n",
    "        keras.layers.MaxPooling2D(2, 2),\n",
    "        keras.layers.Conv2D(64, (3, 3), activation=\"relu\"),\n",
    "        keras.layers.MaxPooling2D(2, 2),\n",
    "        keras.layers.Conv2D(64, (3, 3), activation=\"relu\"),\n",
    "        keras.layers.MaxPooling2D(2, 2),\n",
    "        keras.layers.Conv2D(64, (3, 3), activation=\"relu\"),\n",
    "        keras.layers.MaxPooling2D(2, 2),\n",
    "        keras.layers.Flatten(),\n",
    "        keras.layers.Dense(512, activation=\"relu\"),\n",
    "        keras.layers.Dense(1, activation=\"sigmoid\"),\n",
    "    ]\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d (Conv2D)              (None, 298, 298, 16)      448       \n",
      "_________________________________________________________________\n",
      "max_pooling2d (MaxPooling2D) (None, 149, 149, 16)      0         \n",
      "_________________________________________________________________\n",
      "conv2d_1 (Conv2D)            (None, 147, 147, 32)      4640      \n",
      "_________________________________________________________________\n",
      "max_pooling2d_1 (MaxPooling2 (None, 73, 73, 32)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_2 (Conv2D)            (None, 71, 71, 64)        18496     \n",
      "_________________________________________________________________\n",
      "max_pooling2d_2 (MaxPooling2 (None, 35, 35, 64)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_3 (Conv2D)            (None, 33, 33, 64)        36928     \n",
      "_________________________________________________________________\n",
      "max_pooling2d_3 (MaxPooling2 (None, 16, 16, 64)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_4 (Conv2D)            (None, 14, 14, 64)        36928     \n",
      "_________________________________________________________________\n",
      "max_pooling2d_4 (MaxPooling2 (None, 7, 7, 64)          0         \n",
      "_________________________________________________________________\n",
      "flatten (Flatten)            (None, 3136)              0         \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 512)               1606144   \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 1)                 513       \n",
      "=================================================================\n",
      "Total params: 1,704,097\n",
      "Trainable params: 1,704,097\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(\n",
    "    loss=\"binary_crossentropy\",\n",
    "    optimizer=keras.optimizers.RMSprop(learning_rate=0.001),\n",
    "    metrics=[\"accuracy\"],\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\dell\\.conda\\envs\\dltf\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\training.py:1969: UserWarning: `Model.fit_generator` is deprecated and will be removed in a future version. Please use `Model.fit`, which supports generators.\n",
      "  warnings.warn('`Model.fit_generator` is deprecated and '\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/15\n",
      "33/33 [==============================] - 24s 696ms/step - loss: 0.8539 - accuracy: 0.6018\n",
      "Epoch 2/15\n",
      "33/33 [==============================] - 12s 363ms/step - loss: 0.3515 - accuracy: 0.8763\n",
      "Epoch 3/15\n",
      "33/33 [==============================] - 12s 358ms/step - loss: 0.1751 - accuracy: 0.9357\n",
      "Epoch 4/15\n",
      "33/33 [==============================] - 11s 342ms/step - loss: 0.2180 - accuracy: 0.9455\n",
      "Epoch 5/15\n",
      "33/33 [==============================] - 11s 333ms/step - loss: 0.0990 - accuracy: 0.9649\n",
      "Epoch 6/15\n",
      "33/33 [==============================] - 12s 352ms/step - loss: 0.1652 - accuracy: 0.9708\n",
      "Epoch 7/15\n",
      "33/33 [==============================] - 12s 346ms/step - loss: 0.0305 - accuracy: 0.9932\n",
      "Epoch 8/15\n",
      "33/33 [==============================] - 12s 345ms/step - loss: 0.6679 - accuracy: 0.9786\n",
      "Epoch 9/15\n",
      "33/33 [==============================] - 12s 346ms/step - loss: 0.0019 - accuracy: 1.0000\n",
      "Epoch 10/15\n",
      "33/33 [==============================] - 11s 330ms/step - loss: 0.0491 - accuracy: 0.9922\n",
      "Epoch 11/15\n",
      "33/33 [==============================] - 11s 328ms/step - loss: 0.2851 - accuracy: 0.9766\n",
      "Epoch 12/15\n",
      "33/33 [==============================] - 11s 331ms/step - loss: 0.0044 - accuracy: 0.9990\n",
      "Epoch 13/15\n",
      "33/33 [==============================] - 12s 345ms/step - loss: 0.2364 - accuracy: 0.9659\n",
      "Epoch 14/15\n",
      "33/33 [==============================] - 12s 367ms/step - loss: 0.0030 - accuracy: 0.9990\n",
      "Epoch 15/15\n",
      "33/33 [==============================] - 11s 333ms/step - loss: 1.9396e-04 - accuracy: 1.0000\n"
     ]
    }
   ],
   "source": [
    "history = model.fit_generator(train_generator, epochs=15)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "validation_url = \"https://storage.googleapis.com/laurencemoroney-blog.appspot.com/validation-horse-or-human.zip\"\n",
    "\n",
    "validation_file_name = \"validation-horse-or-human.zip\"\n",
    "validation_dir = \"horse-or-human/validation/\"\n",
    "\n",
    "urllib.request.urlretrieve(validation_url, validation_file_name)\n",
    "\n",
    "zip_ref = zipfile.ZipFile(validation_file_name, \"r\")\n",
    "zip_ref.extractall(validation_dir)\n",
    "zip_ref.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 256 images belonging to 2 classes.\n"
     ]
    }
   ],
   "source": [
    "validation_datagen = image.ImageDataGenerator(rescale=1 / 255.0)\n",
    "\n",
    "validation_generator = validation_datagen.flow_from_directory(\n",
    "    validation_dir, target_size=(300, 300), class_mode=\"binary\"\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/15\n",
      "33/33 [==============================] - 14s 401ms/step - loss: 0.5383 - accuracy: 0.9864 - val_loss: 1.6624 - val_accuracy: 0.8477\n",
      "Epoch 2/15\n",
      "33/33 [==============================] - 13s 379ms/step - loss: 0.0011 - accuracy: 1.0000 - val_loss: 1.7824 - val_accuracy: 0.8711\n",
      "Epoch 3/15\n",
      "33/33 [==============================] - 13s 376ms/step - loss: 0.1322 - accuracy: 0.9873 - val_loss: 0.8549 - val_accuracy: 0.8984\n",
      "Epoch 4/15\n",
      "33/33 [==============================] - 12s 371ms/step - loss: 0.0210 - accuracy: 0.9951 - val_loss: 1.5994 - val_accuracy: 0.8672\n",
      "Epoch 5/15\n",
      "33/33 [==============================] - 13s 381ms/step - loss: 2.4180e-04 - accuracy: 1.0000 - val_loss: 1.7828 - val_accuracy: 0.8906\n",
      "Epoch 6/15\n",
      "33/33 [==============================] - 12s 366ms/step - loss: 0.0812 - accuracy: 0.9903 - val_loss: 2.1352 - val_accuracy: 0.8711\n",
      "Epoch 7/15\n",
      "33/33 [==============================] - 12s 360ms/step - loss: 9.2815e-04 - accuracy: 1.0000 - val_loss: 3.1212 - val_accuracy: 0.8516\n",
      "Epoch 8/15\n",
      "33/33 [==============================] - 13s 377ms/step - loss: 4.0230e-05 - accuracy: 1.0000 - val_loss: 2.8671 - val_accuracy: 0.8633\n",
      "Epoch 9/15\n",
      "33/33 [==============================] - 12s 367ms/step - loss: 6.0816e-06 - accuracy: 1.0000 - val_loss: 2.8113 - val_accuracy: 0.8750\n",
      "Epoch 10/15\n",
      "33/33 [==============================] - 12s 370ms/step - loss: 8.1747e-07 - accuracy: 1.0000 - val_loss: 2.8218 - val_accuracy: 0.8789\n",
      "Epoch 11/15\n",
      "33/33 [==============================] - 12s 388ms/step - loss: 3.0738e-07 - accuracy: 1.0000 - val_loss: 3.0869 - val_accuracy: 0.8867\n",
      "Epoch 12/15\n",
      "33/33 [==============================] - 12s 360ms/step - loss: 4.7532e-08 - accuracy: 1.0000 - val_loss: 3.1426 - val_accuracy: 0.8867\n",
      "Epoch 13/15\n",
      "33/33 [==============================] - 12s 358ms/step - loss: 1.0485e-08 - accuracy: 1.0000 - val_loss: 4.0039 - val_accuracy: 0.8711\n",
      "Epoch 14/15\n",
      "33/33 [==============================] - 12s 372ms/step - loss: 0.1709 - accuracy: 0.9864 - val_loss: 2.9120 - val_accuracy: 0.8516\n",
      "Epoch 15/15\n",
      "33/33 [==============================] - 12s 363ms/step - loss: 0.1193 - accuracy: 0.9873 - val_loss: 2.2388 - val_accuracy: 0.8516\n"
     ]
    }
   ],
   "source": [
    "history = model.fit(\n",
    "    train_generator,\n",
    "    epochs=15,\n",
    "    validation_data=validation_generator\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.]]\n",
      "[0.]\n",
      "is a horse\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.preprocessing import image\n",
    "import numpy as np\n",
    "\n",
    "# 载入图片，并 reshape 为 (300,300)\n",
    "img = image.load_img(\"images/white-horse-gb72298424_640.jpg\", target_size=(300, 300))\n",
    "# 转换为 2D 数组\n",
    "x = image.img_to_array(img)\n",
    "# 扩展维度，转换为 3D 数组\n",
    "x = np.expand_dims(x, axis=0)\n",
    "\n",
    "image_tensor = np.vstack([x])\n",
    "classes = model.predict(image_tensor)\n",
    "print(classes)\n",
    "print(classes[0])\n",
    "\n",
    "if classes[0] > 0.5:\n",
    "    print(\"is a human\")\n",
    "else:\n",
    "    print(\"is a horse\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.]]\n",
      "[0.]\n",
      "is a horse\n"
     ]
    }
   ],
   "source": [
    "# 载入图片，并 reshape 为 (300,300)\n",
    "img = image.load_img(\"images/white-horse-gb72298424_640.jpg\", target_size=(300, 300))\n",
    "# 转换为 2D 数组\n",
    "x = image.img_to_array(img)\n",
    "# 扩展维度，转换为 3D 数组\n",
    "x = np.expand_dims(x, axis=0)\n",
    "\n",
    "classes = model.predict(x)\n",
    "print(classes)\n",
    "print(classes[0])\n",
    "\n",
    "if classes[0] > 0.5:\n",
    "    print(\"is a human\")\n",
    "else:\n",
    "    print(\"is a horse\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "5ca4512713c03c04450e89d6bde36822a6fba07e96534581f08175c8acd38ed8"
  },
  "kernelspec": {
   "display_name": "Python 3.9.9 64-bit ('dltf': conda)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.9"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
