# 梯度下降

- [梯度下降](#梯度下降)
  - [简介](#简介)
  - [梯度](#梯度)
  - [随机梯度下降法](#随机梯度下降法)

2021-08-04, 16:26
***

## 简介

梯度下降是优化机器学习算法最常见的方法之一，用来解决损失函数最小化问题。


最小二乘对应的损失函数为：

$$E(\theta)=\frac{1}{2}\sum_{i=1}^n (y_i-f_{\theta}(x_i))^2$$

其中 $y_i$ 为实际值，$f_{\theta}(x_i)$ 为拟合值。

对回归问题，最小二乘将回归转换为上面函数取最小值的最优化问题。

而对函数求最小值，一般采用微分来求解。例如对函数 $g(x)=(x-1)^2$，其函数图像如下：

<img src="images/2021-02-07-15-40-39.png" width="400">

可以看出其最小值对应 $x=1$。

g(x) 的微分函数为:

$$\frac{d}{dx}g(x)=2(x-1)=2x-2$$

## 梯度

函数的极小值、最小值以及鞍点（saddle point），梯度都为 0：

- 极小值是局部最小值，也就是限定在某个范围内的最小值；
- 鞍点是从某个方向看是极大值，从另一个方向看则是极小值的点。

虽然梯度法是寻找梯度为 0 的地方，但是那个地方不一定是最小值。此外，当函数很复杂且呈扁平状时，学习可能会进入一个几乎平坦的地区，陷入被称为 “学习高原” 的无法前进的停滞期。

虽然梯度的方向不一定指向最小值，但沿着它的方向能够最大限度地减小函数的值。通过不断沿梯度方向前进，逐渐减小函数值得过程就是梯度法（gradient method）。

## 随机梯度下降法

神经网络存在合适的权重和偏置，调整权重和偏置以便拟合训练数据的过程成为“学习”。神经网络的学习分为下面四个步骤：

1. mini-batch

从训练数据种随机选出一部分数据，这部分数据称为 mini-batch。我们的目标是减小 mini-batch 的损失函数的值。

2. 计算梯度

为了减小 mini-batch 的损失函数的值，需要求出各个权重参数的梯度。梯度表示损失函数的值减小最多的方向。

3. 更新参数

将权重参数沿梯度方向进行微小更新。

4. 重复

重复以上步骤。

神经网络的学习按照上面 4 个步骤进行。这个方法通过梯度下降法更新参数，不过因为这里使用的数据是随机选择的 mini batch 数据，所以又称为随机梯度下降法（stochastic gradient descent, SGD）。很多深度学习框架中，随机梯度下降法一般由一个名为 `SGD` 的函数实现。
