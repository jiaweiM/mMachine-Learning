# 正则化

- [正则化](#正则化)
  - [过拟合](#过拟合)
  - [权值衰减](#权值衰减)
  - [Dropout](#dropout)

2021-07-05, 20:14
***

## 过拟合

发生过拟合的原因主要有两个：

- 模型拥有大量参数，表现力强；
- 训练数据少。

## 权值衰减

权值衰减是经常被用来抑制过拟合的方法。该方法通过在学习过程中对大的权重进行惩罚，来抑制过拟合。很多过拟合原本就是因为权重参数取值过大才发生。


## Dropout

作为抑制过拟合的方法，为损失函数加上权重的 L2 范数的权重值衰减方法，该方法实现简单，在某种程度上能够抑制过拟合。但是，如果网络的模型变得很复杂，只用权值衰减就难以应付了。在这种情况下，我们经常会使用 Dropout 方法。

Dropout是一种在学习的过程中随机删除神经元的方法。训练时，随机选出隐藏层的神经元，然后将其删除。被删除的神经元不再进行信号的传递。训练时，每传递一次数据，就会随机选择要删除的神经元。然后，测试时，虽然会传递所有的神经元信号，但是对于各个神经元的输出，要乘上训练时的删除比例后再输出。如下图所示：

![](images/2021-08-13-15-41-53.png)

机器学习中经常使用集成学习。所谓集成学习，就是让多个模型单独进行学习，推理时再取多个模型输出的平均值。用神经网络的语境来说，比如，准备 5 个结构相同（或类似）的网络，分别进行学习，测试时，以这 5 个网络的输出的平均值作为答案。实验告诉我们，通过进行集成学习，神经网络的识别精度可以提高好几个百分点。

集成学习与 Dropout 有密切的关系。这是因为可以将 Dropout 理解为，通过在学习过程中随机删除神经元，从而每一次都让不同的模型进行学习。并且，推理时，通过对神经元的输出乘以删除比例（比如 0.5），可以取得模型的平均值。即，Dropout 将集成学习的效果通过一个网络实现了。

