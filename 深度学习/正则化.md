# 正则化

- [正则化](#正则化)
  - [过拟合](#过拟合)
  - [正则化种类](#正则化种类)
  - [权值衰减](#权值衰减)
  - [Dropout](#dropout)
  - [BatchNormalization](#batchnormalization)

2021-07-05, 20:14
***

## 过拟合

发生过拟合的原因主要有两个：

- 模型拥有大量参数，表现力强；
- 训练数据少。

根据经验，如果在训练过程中，验证数据的损失先减少后开始增加，就可能开始发生过拟合。

模型的复杂度大致可以用 non-zero weights 的数目表示，换句话说，如果两个模型 M1 和 M2 性能接近，那么我们应该选择更简单的模型，即 non-zero weights 更少的模型。

## 正则化种类

在机器学习中使用的正则化方法有三种：

- L1 正则化（L1 regularization），也称为 LASSO，模型的复杂度表示为 weights 的绝对值和；
- L2 正则化（L2 regularization），也称为 Ridge，模型的复杂度表示为 weights 的平方和；
- 弹性正则化（elastic regularization），模型的复杂度由上面两种技术的组合来表示。

在 TensorFlow 中添加正则化的方法：

```py
from tf.keras.regularizers import l2, activity_l2
model.add(Dense(64, input_dim=64, W_regularizer=l2(0.01),
activity_regularizer=activity_l2(0.01)))
```

## 权值衰减

权值衰减是经常被用来抑制过拟合的方法。该方法通过在学习过程中对大的权重进行惩罚，来抑制过拟合。很多过拟合原本就是因为权重参数取值过大才发生。

## Dropout

作为抑制过拟合的方法，为损失函数加上权重的 L2 范数的权重值衰减方法，该方法实现简单，在某种程度上能够抑制过拟合。但是，如果网络的模型变得很复杂，只用权值衰减就难以应付了。在这种情况下，我们经常会使用 Dropout 方法。

Dropout是一种在学习的过程中随机删除神经元的方法。训练时，随机选出隐藏层的神经元，然后将其删除。被删除的神经元不再进行信号的传递。训练时，每传递一次数据，就会随机选择要删除的神经元。然后，测试时，虽然会传递所有的神经元信号，但是对于各个神经元的输出，要乘上训练时的删除比例后再输出。如下图所示：

![](images/2021-08-13-15-41-53.png)

机器学习中经常使用集成学习。所谓集成学习，就是让多个模型单独进行学习，推理时再取多个模型输出的平均值。用神经网络的语境来说，比如，准备 5 个结构相同（或类似）的网络，分别进行学习，测试时，以这 5 个网络的输出的平均值作为答案。实验告诉我们，通过进行集成学习，神经网络的识别精度可以提高好几个百分点。

集成学习与 Dropout 有密切的关系。这是因为可以将 Dropout 理解为，通过在学习过程中随机删除神经元，从而每一次都让不同的模型进行学习。并且，推理时，通过对神经元的输出乘以删除比例（比如 0.5），可以取得模型的平均值。即，Dropout 将集成学习的效果通过一个网络实现了。

## BatchNormalization

BatchNormalization 是另一种形式的正则化，也是最近几年提出的最有效的改进方法之一。BatchNormalization 可以加速训练，早某些情况下可以将训练的 epoch 减半。

在训练过程中，每层的 weights 必须根据不同的 batch 分布不断地重新调整权重，这可能会大大减慢模型的训练速度。BatchNormalization 的核心思想是使输入的分布在不同 batch 以及不同 epoch 更为接近。

另一个问题是 sigmoid 激活函数在零附近工作很好，但当和零很远时，由于导数接近零，训练往往会卡住。如果偶尔出现 neuron 的输出离 0 很远，那么该 neuron 后面将无法更新自己的 wieghts。

因此，可以将 layer 输出转换为接近零的高斯分布，这样不同 batch 之间的 layer 差异将显著减少。从数学上来讲，函数很简单。

