# 激活函数

## ReLU

ReLU（Rectified Linear Unit, 修正线性单元）是目前深度神经网络中最常使用的激活函数。其定义为：

$$
\sigma(x)=max(0,x)=\begin{cases}
x \quad (x>0)\\
0 \quad (x \le 0)
\end{cases}
$$

图示：

![](images/relu.png)

