# 截断 BPTT

- [截断 BPTT](#截断-bptt)
  - [简介](#简介)
  - [TBPTT](#tbptt)
  - [TBPTT 配置对 RNN 序列模型的影响](#tbptt-配置对-rnn-序列模型的影响)
  - [TBPTT 的 Keras 实现](#tbptt-的-keras-实现)
  - [为 TBPTT 准备数据](#为-tbptt-准备数据)
    - [使用原始数据](#使用原始数据)
    - [简单分割](#简单分割)
    - [domain-specific 数据拆分](#domain-specific-数据拆分)
    - [系统数据拆分](#系统数据拆分)
    - [使用 TBPTT(1, 1) 学习内部状态](#使用-tbptt1-1-学习内部状态)
    - [解耦向前和向后序列长度](#解耦向前和向后序列长度)
  - [参考](#参考)

Last updated: 2022-07-29, 09:11
@author Jiawei Mao
****

## 简介

现代循环神经网络，如 LSTM，使用基于时间的反向传播（Backpropagation Through Time, BPTT）进行训练。为了有效地处理长序列预测问题，BPTT 被进一步修改为截断 BPTT 算法（Truncated Backpropagation Through Time, TBPTT）。

使用 TBPTT 训练 RNN 网络时，一个关键参数是输入时间步长。即如何将长序列分割为子序列，以获得最佳性能。

下面介绍如下内容：

- 什么是 TBPTT，以及如何在 Keras 中实现的
- 输入时间步长对 RNN 训练的影响
- 6 种分割长序列的方法，以充分利用 TBPTT 算法训练 RNN

## TBPTT

反向传播（BPTT）用于更新神经网络权重的训练算法，以最小化给定输入下的预期输入和预期输出之间的误差。

对观测值之间存在顺序依赖的序列预测问题，使用循环神经网络代替经典的前馈神经网络。循环神经网络使用反向传播算法的一种变体，基于时间的反向传播（BPTT）进行训练。

实际上，BPTT 展开 RNN 网络，并在整个网络上反向传播误差，每次一个时间步。然后根据累计梯度更新权重。

当输入序列很长时，BPTT 训练 RNN 的速度很慢。另外，在很多时间步上累计梯度，可能导致梯度爆炸或梯度消失。

BPTT 的一种变体是限制反向传播的时间步长，并使用限制的步长计算梯度，更不是计算整个序列的梯度。

这个变体称为基于时间的截断反向传播（Truncated Backpropagation Through Time, TBPTT）。

TBPTT 训练算法包含两个参数：

- `k1`：正向传播时的时间步数
- `k2`：反向传播计算梯度时使用的时间步数

因此，用 `TBPTT(k1, k2)` 表示 TBPTT 算法配置，当 $k_1 = k_2 = n$ 时，就是经典的非截断 BPTT 算法，其中 n 是完整输入序列长度。

## TBPTT 配置对 RNN 序列模型的影响

现代 RNN 网络（如 LSTM）可以利用它们的内部状态记忆很长的输入序列，如上千个时间步。因此，TBPTT 的参数不一定要用来定义网络记忆，而是影响网络如何计算更来更新权重的梯度。具体地说，TBPTT 的参数定义了网络用来模拟序列问题的时间步长度。

正式定义：

$$\hat{y} = f(X(t), X(t-1), X(t-2), ..., X(t-n))$$

其中 $\hat{y}$ 是特定时间步的输出，$f(...)$ 是 RNN 网络近似出的关系，$X(t)$ 是特定时间步的观测值。

## TBPTT 的 Keras 实现

Keras 深度学习库提供了 TBPTT 的实现，其实现比上面的通用定义更局限。具体来说，就是 $k_1$ 和 $k_2$ 必须相等。

Keras 通过固定 RNN（如 LSTM）的三维输入来实现的，LSTM 要求输入数据的 shape 为：samples, timesteps, features。

其中 `timesteps` 定义序列预测问题的向前和向后传播的时间步数，因此，在为 Keras 的序列预测问题准备输入数据时，要仔细选择时间步数。

时间步数的影响有两个方面：

- 正向传播时累计的内部状态
- 反向传播用于更新权重的梯度值计算

需要注意的是，在默认情况下，RNN 网络的内部状态在每个 batch 后重置，但可以通过使用 LSTM 的 `stateful` 参数和 `reset` 操作手动控制该重置。

## 为 TBPTT 准备数据

### 使用原始数据

如果输入序列的时间步数适中，就可以直接使用原始数据。

建议 TBPTT 的长度不超过 200-400 时间步。

如果序列长度小于该范围，则可以将数据直接 reshape 所需输入数据。

例如，如果你有一个 25 个时间步长的 100 个单变量序列集合，则可以将其 reshape 为 100 samples, 25 timesteps, 1 feature 的数据集 [100, 25, 1]。

### 简单分割

如果输入序列很长，例如包含上千个时间步，则可能需要将长序列拆分为多个连续的子序列。

这需要在 Keras 使用 `stateful` 的 `LSTM`，以便在子序列输入时维持内部状态，并且仅在完整的输入序列结束时重置状态。

例如，假设你有 100 条长度为 50,000 的序列，则可以将每个输入序列拆分为 100 个长度为 500 的子序列。这样一个序列变为 100 个 sampels，原始的 100 个 样本变为 10,000 个样本。此时 Keras 的输入 shape 应为 [10000, 500, 1]。这个原始样本的 100 条序列中要保持状态 `stateful`，并在结束时重置状态，可以通过显式调用方法实现，也可以将 batch size 设置为 100。

推荐将整个序列拆分为等大小的子序列。子序列的长度选择比较随意，所以称为**简单分割**。

这种方式在拆分序列时没有考虑子序列长度对模型的影响。

### domain-specific 数据拆分

很难知道哪个时间步数最有利于梯度的计算，我们可以用 Naive 方法快速获得模型，但是模型性能可能远未最优。

或者，使用领域特定知识（domain specific）来估计适合问题的时间步数。例如，如果序列问题是一个回归问题，那么自相关图（autocorrelation）可用来辅助选择时间步长。

如果序列问题是一个 NLP 问题，那么可以将输入序列按句子进行拆分，然后填充到固定长度，或者根据该领域的平均句子长度进行拆分。

### 系统数据拆分

可以通过系统的评估找到合适的时间步长。

例如，在不同子序列长度上执行网格搜索（grid search），采用平均结果最好的时间步长。

如果采用这种方法，需要注意以下几点：

- 以全序列长度的因子作为起始搜索长度
- 如果子序列长度不是全长的因子，使用 padding 或 masking
- 对不同配置，采用多次运行（如 30）的平均性能

如果计算资源重组，则推荐该方法。

### 使用 TBPTT(1, 1) 学习内部状态

可以将序列预测问题调整为每个时间步包含一个输入和一个输出的问题。

例如，如果你有 100 个长度为 50 的序列，将每个时间步作为 1 个新的样本，则 100 个样本变为 5,000 个。此时输入 shape 为 [5000, 1, 1]。

同样，此时需要每 50 个样本保持状态。

根据经验，这种方式对需要记忆序列的预测问题效果还行，但如果输出是过去时间步的复杂函数，效果较差。

### 解耦向前和向后序列长度

Keras 支持解耦 TBPTT 向前传播和向后传播的时间步长。

`k1` 参数可以通过输入序列的时间步长指定，`k2` 可以通过 LSTM 的 `truncated_gradient` 参数指定。



## 参考

- https://rramosp.github.io/2021.deeplearning/content/U5.03%20-%20Truncated%20BPTT.html
- https://machinelearningmastery.com/truncated-backpropagation-through-time-in-keras/
