# 批量归一化

## 简介

对比较深的神经网络，假设数据在底部，因为损失在顶部，从上到下开始计算梯度和更新参数：

- 底部的层训练较慢
- 底部层一变化，所有都得跟着变，顶部的层就需要重新学习
- 最终导致收敛变慢

批量归一化（Batch Normalization）固定小批量里面的均值和方差：

批量归一化可学习的参数：$\gamma$ 和 $\beta$。

应用：

- 全连接层和卷积层输出上，激活函数前
- 全连接层和卷积层输入上
- 对全连接层，作用在特征维
- 对卷积层，作用在通道维

总结：

- 批量归一化固定小批量中的均值和方差，然后学习出适合的偏移和缩放
- 可以加速收敛速度，但一般不改变模型精度

## PyTorch 实现



## 参考

- https://zh-v2.d2l.ai/chapter_convolutional-modern/batch-norm.html
