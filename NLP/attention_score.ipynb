{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from d2l import tensorflow as d2l"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# @save\n",
    "def masked_softmax(X, valid_lens):\n",
    "    \"\"\"通过在最后一个轴上掩蔽元素来执行softmax操作\"\"\"\n",
    "    # X:3D张量，valid_lens:1D或2D张量\n",
    "    if valid_lens is None:\n",
    "        return tf.nn.softmax(X, axis=-1)\n",
    "    else:\n",
    "        shape = X.shape\n",
    "        if len(valid_lens.shape) == 1:\n",
    "            valid_lens = tf.repeat(valid_lens, repeats=shape[1])\n",
    "\n",
    "        else:\n",
    "            valid_lens = tf.reshape(valid_lens, shape=-1)\n",
    "        # 最后一轴上被掩蔽的元素使用一个非常大的负值替换，从而其softmax输出为0\n",
    "        X = d2l.sequence_mask(\n",
    "            tf.reshape(X, shape=(-1, shape[-1])), valid_lens, value=-1e6\n",
    "        )\n",
    "        return tf.nn.softmax(tf.reshape(X, shape=shape), axis=-1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(2, 2, 4), dtype=float32, numpy=\n",
       "array([[[0.33927715, 0.6607229 , 0.        , 0.        ],\n",
       "        [0.5186399 , 0.48136008, 0.        , 0.        ]],\n",
       "\n",
       "       [[0.2706828 , 0.34843233, 0.38088483, 0.        ],\n",
       "        [0.28000477, 0.29489115, 0.42510405, 0.        ]]], dtype=float32)>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "masked_softmax(tf.random.uniform(shape=(2, 2, 4)), tf.constant([2, 3]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(2, 2, 4), dtype=float32, numpy=\n",
       "array([[[1.        , 0.        , 0.        , 0.        ],\n",
       "        [0.30077505, 0.25723618, 0.4419887 , 0.        ]],\n",
       "\n",
       "       [[0.5924722 , 0.40752774, 0.        , 0.        ],\n",
       "        [0.19095424, 0.21986324, 0.22264503, 0.36653754]]], dtype=float32)>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "masked_softmax(tf.random.uniform(shape=(2, 2, 4)), tf.constant([[1, 3], [2, 4]]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# @save\n",
    "class AdditiveAttention(tf.keras.layers.Layer):\n",
    "    \"\"\"Additiveattention.\"\"\"\n",
    "\n",
    "    def __init__(self, key_size, query_size, num_hiddens, dropout, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        self.W_k = tf.keras.layers.Dense(num_hiddens, use_bias=False)\n",
    "        self.W_q = tf.keras.layers.Dense(num_hiddens, use_bias=False)\n",
    "        self.w_v = tf.keras.layers.Dense(1, use_bias=False)\n",
    "        self.dropout = tf.keras.layers.Dropout(dropout)\n",
    "\n",
    "    def call(self, queries, keys, values, valid_lens, **kwargs):\n",
    "        queries, keys = self.W_q(queries), self.W_k(keys)\n",
    "        # 在维度扩展后，\n",
    "        # queries的形状：(batch_size，查询的个数，1，num_hidden)\n",
    "        # key的形状：(batch_size，1，“键－值”对的个数，num_hiddens)\n",
    "        # 使用广播方式进行求和\n",
    "        features = tf.expand_dims(queries, axis=2) + tf.expand_dims(keys, axis=1)\n",
    "        features = tf.nn.tanh(features)\n",
    "        # self.w_v仅有一个输出，因此从形状中移除最后那个维度。\n",
    "        # scores的形状：(batch_size，查询的个数，“键-值”对的个数)\n",
    "        scores = tf.squeeze(self.w_v(features), axis=-1)\n",
    "        self.attention_weights = masked_softmax(scores, valid_lens)\n",
    "        # values的形状：(batch_size，“键－值”对的个数，值的维度)\n",
    "        return tf.matmul(self.dropout(self.attention_weights, **kwargs), values)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# @save\n",
    "class DotProductAttention(tf.keras.layers.Layer):\n",
    "    \"\"\"Scaleddotproductattention.\"\"\"\n",
    "\n",
    "    def __init__(self, dropout, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        self.dropout = tf.keras.layers.Dropout(dropout)\n",
    "\n",
    "    # queries的形状：(batch_size，查询的个数，d)\n",
    "    # keys的形状：(batch_size，“键－值”对的个数，d)\n",
    "    # values的形状：(batch_size，“键－值”对的个数，值的维度)\n",
    "    # valid_lens的形状:(batch_size，)或者(batch_size，查询的个数)\n",
    "    def call(self, queries, keys, values, valid_lens, **kwargs):\n",
    "        d = queries.shape[-1]\n",
    "        scores = tf.matmul(queries, keys, transpose_b=True) / tf.math.sqrt(\n",
    "            tf.cast(d, dtype=tf.float32)\n",
    "        )\n",
    "        self.attention_weights = masked_softmax(scores, valid_lens)\n",
    "        return tf.matmul(self.dropout(self.attention_weights, **kwargs), values)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.10 ('d2l')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.10"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "3e977ccf7b4f79a7e503365935f6fdfa6785c3fffe784e1a4df4295a6c70decf"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
