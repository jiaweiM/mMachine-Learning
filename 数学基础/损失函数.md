# 损失函数

- [损失函数](#损失函数)
  - [简介](#简介)
  - [0-1 损失函数](#0-1-损失函数)
  - [均方差](#均方差)
  - [交叉熵损失函数](#交叉熵损失函数)
  - [Hinge 损失函数](#hinge-损失函数)
  - [参考](#参考)

2021-05-27, 15:32
***

## 简介

损失函数是一个非负实数函数，用来量化模型预测和真实标签之间的差异。也称为代价函数，目标函数。

机器学习训练算法的目的就是为了最小化损失函数，获得对应的参数值。

## 0-1 损失函数

0-1损失函数是最直观的损失函数，表征模型在训练集上的错误率：

$$
\begin{align}
L(y.f(x;\theta)) &= \begin{cases}
0 &\text{if} \quad y = f(x;\theta)\\
1 &\text{if} \quad y \ne f(x;\theta)
\end{cases}\\
&=I(y\ne f(x;\theta))
\end{align}
$$

其中 $I(\cdot)$ 是[指示函数](数学基础/../函数基础.md#指示函数)。

虽然 0-1 损失函数能够客观评价模型的好坏，但数学性质不好：不连续且导数为 0，难以优化。因此经常使用连续可微的损失函数替代。

大多数情况下，权重和偏置的微小变化不会影响正确分类的数量，因此采用正确分类数作为指标很难通过改变权重和偏执提升表现，而平滑的损失函数能更好通过微调权重和偏置来改善效果。

## 均方差

平方损失函数（Quadratic Loss Function）经常用在预测标签 y 为实数值的任务中，定义为：

$$
L(y,f(x;\theta)) = \frac{1}{2} (y-f(x;\theta))^2
$$

平方损失函数一般不适用于分类问题。

## 交叉熵损失函数

交叉熵误差公式如下：

$$
E = -\sum_k t_klogy_k
$$

这里 $y_k$ 是模型预测值，$t_k$ 是正确解标签。并且 $t_k$ 为 one-hot 表示，即只有一个值为 1，其他为 0。所以上式只需要计算正解标签的输出的自然对数。

交叉熵损失函数（Cross-Entropy Loass Function）一般用于分类问题。假设样本的标签 $y \in \{1,...,C\}$为离散的类别，模型 $f(x;\theta) \in [0,1]^C$ 的输出为类别标签的条件概率分布，即：

$$p(y=c|x;\theta)=f_c(x;\theta)$$

> $f_c(x;\theta)$ 表示 $f(x;\theta)$的输出向量的第 c 维。

并满足：

$$f_c(x;\theta)\in [0,1],  \sum_{c=1}^Cf_c(x;\theta)=1$$

我们可以用一个 C 维的 [one-hot 向量](函数基础.md#one-hot-向量) y 来表示样本标签。假设样本的标签为 k，那么标签向量 y 只有第 k 维的值为 1，其余元素的值为 0。标签向量 y 可以看作样本标签的真实条件概率分布 $p_r(y|x)$，即第 c 维（记为$y_c$，$1\le c \le C$）是类别为 c 的真实条件概率。假设样本的类别为 k，那么它属于第 k 类的概率为 1，属于其他类的概率为 0.

对于两个概率分布，一般可以用交叉熵来衡量它们的差异。标签的真实分布 y  和模型预测分布 $f(x;\theta)$之间的交叉熵为：

$$\begin{align}
L(y,f(x;\theta)) &= -y^Tlogf(x;\theta)\\
&= -\sum_{c=1}^C y_clogf_c(x;\theta)
\end{align}$$

比如对于三分类问题，一个样本的标签向量为 $y=[0,0,1]^T$，模型预测的标签分布为 $f(x;\theta)=[0.3,0.3,0.3]^T$，则它们的交叉熵为 $-(0\times log(0.3)+0\times log(0.3)+1\times log(0.4))=-log(0.4)$.

因为 y 为 one-hot 向量，所以交叉熵可以简化为：

$$L(y,f(x;\theta))=-logf_y(x;\theta)$$

其中 $f_y(x;\theta)$可以看作真实类别 y 的似然函数。因此，交叉熵损失函数也就是负对数似然函数（Negative Log-likelihood）。

## Hinge 损失函数

对于二分类问题，假设 y 的取值为 $\{-1,+1\}$，$f(x;\theta)\in R$，Hinge 损失函数（Hinge Loss Function）为：

$$\begin{align}
L(y,f(x;\theta)) &= max(0, 1-yf(x;\theta)) \\
&\triangleq [1-yf(x;\theta)]_+
\end{align}$$

其中 $[x]_+=max(0,x)$。

## 参考

- 神经网络与深度学习，邱锡鹏
