# 超参数选择

## 优化算法

小批量随机梯度下降是深度学习默认的求解算法。

虽然还有更好的，但是 SGD 是最稳定的，最简单的。

收敛快不快不是很重要，关键是收敛到什么地方。

## 学习率

如何选择学习率：

- 选择对学习率不敏感的算法，比如 adam
- 合理初始化参数，学习率选择 0.01 或者 0.1，影响不大

## 批量大小

**不能太小**

每次计算量太小，不适合并行来最大利用计算资源。

**不能太大**

内存消耗增加；浪费计算，例如，如果所有样本都是相同的。

batch size 小一点问题不大，batch size 小的时候

噪音对神经网络是好事情，一定的噪音，可以让训练不走偏。