# 高斯混合模型

## 简介

高斯混合模型（Gaussian Mixture Model, GMM）是一种概率模型，用于表示总体中正态分布的亚群合集。混合模型通常不需要知道数据点属于哪个亚群，模型可以自己学习亚群类型。由于亚群类型是未知的，所以 GMM 是一种无监督学习。

例如，建模人的身高数据，身高一般按照性别分别建模为正态分布，男性平均身高约为 169 cm，女性平均身高约为 158 cm。如果只有身高数据而没有性别信息，那么所有身高的分布服从两个不同缩放（方差）和偏移（均值）的正态分布。GMM 模型即处理这类问题，通常 GMM 超过两个组分。估计各个正态分布组分的参数是 GMM 建模的典型问题。

GMM 已被用于语音数据的特征提取，也被广泛用于多目标中的目标跟踪，视频的每一帧混合组分个数对应目标个数，均值对应的对象位置。

## 多变量正态分布

多变量正态分布（multivariate normal distribution）是对正态分布的泛化，对 $p$ 个变量 $X_1, X_2,...,X_p$，多正态分布由均值 $\mu=\mu_1,\mu_2,...,\mu_p$ 和协方差矩阵 $\sum$ 定义。

## 动机

数据服从混合模型，表示数据是多模态的，即在数据分布中有多个峰。试图用单峰模型来拟合多模态分布，结果往往很差（如下图所示）。许多简单分布是单模态的，因此模拟多模态分布的一个直接方法是假设它由多个单模态分布生成。基于一些理论原因，高斯分布是模拟真实单模态数据最常用的分布。因此，将多模态数据建模为多个单模态高斯分布的混合就比较直观。此外，GMM 保留了高斯模型的许多理论和计算优势，使它能够有效地建模非常大的数据集。

![[Pasted image 20230529230203.png]]
> 左：用单个高斯分布拟合；右：用双组分高斯混合模型拟合

## 模型

GMM 包含两种类型的参数：混合组分权重以及各组分的均值和方差/协方差。对包含 K 个组分的 GMM，对单变量情况，第 k 个组分的均值为 $\mu_k$，方差为 $\sigma_k$；对多变量情况，第 k 个组分均值为 $\overrightarrow{\mu}_k$ ，协方差矩阵为 $\sum_k$。组分 $C_k$ 的混合组分权重设为 $\phi_k$，其中 $\sum^{K}_{i=1}\phi_i=1$，即总的概率分布加和为 1。如果组分的权重没有被学习到，它们可以视为组分的先验分布，即 $p(x 由组分 C_k 生成)=\phi_k$。如果学习到组分权重，那么它们就是给定数据下组分概率的后验估计。

**一维模型**

$$p(x)=\sum^{K}_{i=1}\phi_i N(x|\mu_i, \sigma_i)$$

$$N(x|\mu_i,\sigma_i)=\frac{1}{\sigma_i\sqrt{2\pi}}exp(-\frac{(x-\mu_i)^2)}{2\sigma_i^2})$$
$$\sum^{K}_{i=1}\phi_i=1$$
**多维模型**

$$p(\vec{x})=\sum^{K}_{i=1}\phi_iN(\vec{x}|\vec{\mu_i},\sum_i)$$
$$N(\vec{x}|\vec{\mu_i},\sum_i)=\frac{1}{\sqrt{(2\pi)^K|\sum_i|)}}exp(-\frac{1}{2}(\vec{x}-\vec{\mu_i})^T\sum_i^{-1}(\vec{x}-\vec{\mu_i}))$$

$$\sum^{K}_{i=1}\phi_i=1$$

## 学习模型

如果模型组分数 $K$ 已知，期望极大化（Expectation Maximization, EM）是估计混合模型参数最常用的技术。 在概率论中，模型使用最大似然估计来学习，即在给定模型参数下最大化观测数据的概率/似然。不幸的是，混合模型的 log 概率通常没有解析解。

EM 是一种用于最大似然估计的数值计数。EM 是一种迭代算法，随着迭代进行增加数据的极大似然值，即保证接近局部最大值或鞍点。

### EM 用于 GMM

混合模型的 EM 包括两步：

1. 期望（expectation）步或 E 步，给定模型参数 $\phi_k$, $\mu_k$ 和 $\sigma_k$，计算每个数据点 $x_i\in X$ 归属每个组件 $C_k$ 的期望值；
2. 极大（maximization）步或 M 步，根据模型参数最大化 E 步的期望值。该步骤涉及更新 $\phi_k$, $\mu_k$ 和 $\sigma_k$ 的值。

重复执行整个迭代过程，直到算法收敛。从直观上来讲，该算法之所以有效，是因为知道每个 $x_i$ 对 $C_k$ 的分量使得求解 $\phi_k$, $\mu_k$ 和 $\sigma_k$ 变得容易，而知道  $\phi_k$, $\mu_k$ 和 $\sigma_k$ 后计算 $p(C_k|x_i)$ 也容易。E 步对应后一个情况，M 步对应前者。因此，通过交替假定已知的参数，可以有效地计算未知参数的极大似然估计。

### 单变量 GMM

GMM 的 E 步从初始化步骤开始，即根据数据为模型分配一个初始值。然后，迭代 E 步和 M 步，直到参数估计收敛，即迭代 $t$ 时所有参数 $\theta_t$，$|\theta_t-\theta_{t-1}|\le \epsilon$ ，其中 $\epsilon$ 是用于自定义阈值。下图是两组分、双变量 GMM 模型的 EM 算法图示：

![[clustering_of_old_faithful_data.gif]]




## 参考

- https://brilliant.org/wiki/gaussian-mixture-model/