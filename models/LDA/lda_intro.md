# 线性判别分析概述

- [线性判别分析概述](#线性判别分析概述)
  - [简介](#简介)
  - [PCA vs. LDA](#pca-vs-lda)
  - [线性判别分析流程](#线性判别分析流程)
    - [计算散点矩阵](#计算散点矩阵)

@author Jiawei Mao
***

## 简介

LDA 可以看作一种的特征提取技术，能够提高计算效率，降低非正则化模型中维度诅咒导致的过拟合。LDA 的概念与 PCA 相似，不过 PCA 试图找到数据集中方差最大的正交分量轴，而 LDA 的目标是找到能够最好区分类别的特征子空间。

## PCA vs. LDA

PCA 和 LDA 都是线性变换技术，可以用来减少数据集的维度；其中 PCA 是一种无监督算法，而 LDA 是一种有监督算法。与 PCA 相比，我们可以认为 LDA 是一种用于分类任务的特征提取技术。

!!! note
    LDA 也称为 Fisher's LDA。Ronald A. Fisher 在 1936 年首次提出二分类问题的 Fisher 判别分析。1948 年，C. Radhakrishna Rao 在类协方差相等、且满足正态分布的假设下将 Fisher 判别分析推广到多分类问题，即现在的 LDA。

下图总结了二分类问题中的 LDA 概念。

![](images/2023-12-11-10-33-12.png){width="360"}

> 类别 1 用圈表示，类别 2 用叉表示

线性判别，如 x 轴（LD 1）所示，可以很好地分离两个正态分布的类别。尽管 y 轴（LD 2）捕获数据集中许多方差，但它没有捕获任何类别信息，因此不是一个好的线性判别。

LDA 的一个假设是数据是正态分布的。此外，还假设各个类别有相同的协方差矩阵，并且训练样本在统计上彼此独立。不过，即使违背这些假设中的一个或多个，LDA 仍然可以很好地降维。

## 线性判别分析流程

LDA 的主要步骤：

1. 标准化 d 维数据集（d 是特征数）
2. 对每个类别，计算 d 维均值向量
3. 构造类间散点矩阵 $S_B$，和类内散点矩阵 $S_W$。
4. 计算矩阵的特征向量（eigenvector）和对应的特征值 $S_W^{-1}S_B$
5. 对特征值降序排序，以对相应的特征向量进行排序
6. 选择 k 个特征值最大的特征向量，构造 $d\times k$ 变换矩阵 **W**;特征向量为该矩阵的列
7. 使用变换矩阵 **W** 将样本投影到新的特征子空间

LDA 与 PCA 非常相似，我们将矩阵分解为特征值和特征向量，它们将形成新的低维特征空间。不过，LDA 考虑了类标签信息，这些信息在步骤 2 中计算平均向量的形式存在。

### 计算散点矩阵


