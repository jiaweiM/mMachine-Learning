# 稳健回归（Robust Regression）

## 简介

我们已经了解普通最小二乘来估计回归线，然而，普通最小二乘无法处理非恒定方差和异常值问题，此时需要不同的方法来估计回归线。普通最小二乘描述：

$$
\begin{align*} \hat{\beta}_{\textrm{OLS}}&=\arg\min_{\beta}\sum_{i=1}^{n}\epsilon_{i}^{2} \\ &=(\textbf{X}^{\textrm{T}}\textbf{X})^{-1}\textbf{X}^{\textrm{T}}\textbf{Y} \end{align*}
$$

稳健回归试图克服传统回归分析的一些局限性。回归分析对一个或多个自变量和一个因变量之间的关系进行建模。标准的回归方法，如谱图最小二乘，如果其基本假设为真，就能获得较好的属性，否则可能给出误导性的结果。稳健回归方法旨在限制违反假设的数据（离群值）对回归估计的影响。

例如，最小二乘回归对离群值非常敏感：一个误差幅度是典型观测值两倍的离群值，对平方误差损失的贡献是 4 倍，因此对回归估计的影响更大。[Huber 损失函数](https://en.wikipedia.org/wiki/Huber_loss)可替代标准平方误差，它减少了离群值对平方误差的贡献，从而限制它们对回归估计的影响。

## 应用

### 异方差误差

当强烈怀疑存在异方差（heteroscedastic error）时，就要考虑采用稳健估计。

**同方差模型**假定误差项的方差对所有 $x$ 都是常数；而异方差允许方差依赖于 $x$，这更符合大多实际情况。例如，高收入人群的支出方差往往大于低收入人群。软件包通常默认使用同方差模型，尽管可能不如异方差模型准确。

一种简单方法（Tofallis, 2008）是对**百分误差**应用最小二乘法，与普通最小二乘法相比，这样可减少因变量的较大值造成的影响。

### 离群值

使用稳健估计的另一种常见情况是数据包含离群值。

若存在于其它数据生成过程不同的离群值，最小二乘法估算的效率会很低，且会产生偏差。由于最小二乘预测结果会被拖向离群值，且估计值的方差也会被扩大，结果就是**异常值被掩盖**。而许多时候，在地理统计和**医学统计**等领域，感兴趣的恰恰是离群值。

尽管有人称最小二乘法（或一般的经典统计方法）是稳健的，但这个稳健只是指在违反模型的情况下 I 类错误率（false positive）不会增加。实际上，当存在离群值时，第 I 类错误率往往会低于定类水平（nominal level），而第 II 类错误率（false negative）则会急剧上升。第 I 类错误率的下降被称为经典方法的保守性。

## 稳健回归历史

虽然稳健回归在很多时候都优于最小二乘法，但仍未得到广泛应用。

不受欢迎的原因有多个：

1. 多种方法相互竞争，且该领域开局有多个错误；
2. 稳健回归的计算量比最小二乘法大得多。近年来，随着算力的大幅提高，该原因已经不重要了；
3. 一些流行统计软件包没能实现这些方法；

尽管稳健方法的应用进展缓慢，但现代的主流统计学教科书通常都有对这些方法的讨论，如 Seber & Lee 及 Faraway 的著作；关于各种稳健回归方法如何发展的概述，请参阅 Andersen 的著作。此外，现代统计学软件包，如 R, Statsmodels, Stata 和 S-PLUS 包含相当多的稳健估计功能，具体参考 Venables & Ripley & Maronna 的著作。

## 稳健回归方法

### 替换最小二乘

最简单的方法是使用 L1 损失替代 L2 损失，因为 L1 损失对离群值没有 L2 损失敏感。即便如此，严重的离群值仍会对模型产生相当大的影响，促使人们研究更加稳健的方法。

1964 年，Huber 在回归中引入了 [M-估计](https://en.wikipedia.org/wiki/M-estimator)，其中 M 代表最大似然类型（maximum likelihood type）。该方法对对响应变量中的异常值很稳健，但对解释变量（即自变量）的异常值则无能为力。事实上，当解释变量中存在离群值，这种方法与最小二乘相比没有任何优势。

1980 年代，为了克服 M-估计对解释变量离群值不稳健的问题，人们提出了多种方案。Rousseeuw & Leroy 的著作有详细的综述：

- 最小截平方（Least trimmed squares, LTS）是一种可行的替代方案，目前 (2007) 是 Rousseeuw & Ryan (1997, 2008)的首选。
- Theil-Sen 估计泰尔-森估算的分解点低于 LTS，但在统计上很有效，也很受欢迎；
- 还有 S-估计，该方法能找到一条线（面或超平面），使残差规模的稳健估计值（名称出处）最小化。这种方法对杠杆点有很强抵抗力，对响应中的离群值也很稳健，但往往很低效；

MM-估计试图保留S-估计的稳健性，同时获得 M-估计的效率。该方法首先找到一个十分稳健、抗干扰的S-估计值，可使残差尺度的M估计值（第一个M）最小化。然后，在确定参数的M估计值（第二个M）的同时，保持估计值不变。

### 参数替代

另一种稳健估计回归方法是用**重尾分布**代替正态分布。据报道，在各种实际情况下，自由度 4~6 的T分布都是不错的选择。完全参数化的贝叶斯稳健回归，在很大程度上依赖于这类分布。

假设残差为 t-分布，分布是一个位置尺度族，即 $x\leftarrow (x-\mu)/\sigma$。t分布的自由度，有时也称为峰度系数。Lange、Little & Taylor (1989)从非贝叶斯的角度深入讨论了这一模型；Gelman et al. (2003)对贝叶斯模型进行了阐述。

另一种参数方法是假设残差遵循混合正态分布（Daemi et al. 2019）；特别是污染正态分布，其中大部分观测值来自指定的正态分布，小部分来自方差大得多的正态分布。即，残差来自方差为
�
2
{\displaystyle \sigma ^{2}}的正态分布的概率为
1
−
�{\displaystyle 1-\varepsilon }，其中
�{\displaystyle \varepsilon }很小，而对某个
�
>
1
{\displaystyle c>1}，来自方差为
�
�
2
{\displaystyle c\sigma ^{2}}的正态分布的概率为
�{\displaystyle \varepsilon }：

�
�
∼
(
1
−
�
)
�
(
0
,
�
2
)
+
�
�
(
0
,
�
�
2
)
.
{\displaystyle e_{i}\sim (1-\varepsilon )N(0,\sigma ^{2})+\varepsilon N(0,c\sigma ^{2}).}
通常有
�
<
0.1
{\displaystyle \varepsilon <0.1}。这有时被称为
�{\displaystyle \varepsilon }污染模型。

参数法的优点是，由似然理论提供了一种“现成”的推断方法（虽然对
�{\displaystyle \varepsilon }污染模型之类不适用通常的正则行条件），且可根据拟合结果建立模拟模型。但这种参数模型仍假定基本模型是真实的，因此不能考虑偏移的残差分布或有限的观测精度。

## 包含离群值的回归

有许多线性回归算法，它们假设输入变量和输出变量之间是线性关系，如二维的直线，三维的平面和高维的超平面。对许多预测任务来说，这是一个合理的假设。

线性回归假设每个变量的概率分布是有规律的。数据集特征的概率分布越差，线性回归找到良好拟合的可能性越低。

当使用线性回归时，变量概率分布的一个主要问题是**离群值**（outlier）。离群值远离预期分布。例如，如果一个变量服从高斯分布，那么和均值相差 3 到 4 个标准差的数据就是离群值。

数据集的输入变量和目标变量都可能有离群值，且都会导致线性回归出问题。

离群值会扭曲变量的统计量，如均值和标准差，从而导致模型向离群值偏移，远离观测中心。鲁棒线性回归，对经典线性回归进行修改，以处理离群值问题。

## 包含离群值的数据集

可以使用 `sklearn.datasets.make_regression()` 函数生成一个回归数据集。

为了便于可视化，这里生成的数据集，只有一个输入变量和一个输出变量。为了不让任务太简单，还添加统计噪声：

```python
X, y = make_regression(n_samples=100, n_features=1, tail_strength=0.9, effective_rank=1,
                       n_informative=1, noise=3, bias=50, random_state=1)
```

然后，在输入数据中添加异常值。可以通过修改一些输入变量来实现，使其与均值相差 factor 个标准差。这里添加 10 个离群值：

```python

```

## 参考

- https://online.stat.psu.edu/stat501/lesson/topic-1-robust-regression
- https://en.wikipedia.org/wiki/Robust_regression